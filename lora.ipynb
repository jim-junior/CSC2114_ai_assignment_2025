{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d49c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q diffusers==0.19.3 transformers accelerate safetensors huggingface_hub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a210d",
   "metadata": {},
   "source": [
    "# Mount data from google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a810fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # authorize when prompted\n",
    "\n",
    "# Example paths (adjust if your data is stored elsewhere in Drive)\n",
    "# If you uploaded to Drive:\n",
    "DATA_ROOT = \"/content/drive/MyDrive/sd_dataset\"   # <-- change this path\n",
    "# Or, if you uploaded directly into the session:\n",
    "# DATA_ROOT = \"/content/dataset\"\n",
    "\n",
    "# For this notebook we expect:\n",
    "TRAIN_IMAGES = f\"{DATA_ROOT}/train/images\"\n",
    "CAPTIONS_JSON = f\"{DATA_ROOT}/captions.json\"\n",
    "\n",
    "print(\"Train images folder:\", TRAIN_IMAGES)\n",
    "print(\"Captions JSON:\", CAPTIONS_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59d8a14",
   "metadata": {},
   "source": [
    "# Login to Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4353e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = input(\"Paste your Hugging Face token (read access): \").strip()\n",
    "login(hf_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bc6e3",
   "metadata": {},
   "source": [
    "# Run training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63325baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example run (execute in a code cell)\n",
    "!python train_lora_colab.py \\\n",
    "  --images_dir \"$TRAIN_IMAGES\" \\\n",
    "  --captions_json \"$CAPTIONS_JSON\" \\\n",
    "  --output_dir \"/content/outputs/lora_event\" \\\n",
    "  --resolution 512 \\\n",
    "  --train_batch_size 1 \\\n",
    "  --learning_rate 1e-4 \\\n",
    "  --max_train_steps 1200 \\\n",
    "  --lora_rank 4 \\\n",
    "  --lora_alpha 16.0 \\\n",
    "  --save_every 300 \\\n",
    "  --log_every 20 \\\n",
    "  --merge_lora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e9f8cd",
   "metadata": {},
   "source": [
    "# Inference: load merged model and generate images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27e8b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import StableDiffusionPipeline, UNet2DConditionModel, AutoencoderKL\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "base_model = \"runwayml/stable-diffusion-v1-5\"\n",
    "merged_unet_dir = \"/content/outputs/lora_event/merged_unet\"  # produced by --merge_lora\n",
    "\n",
    "# Load base pipeline then replace UNet with merged one\n",
    "pipe = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16 if device=='cuda' else torch.float32)\n",
    "merged_unet = UNet2DConditionModel.from_pretrained(merged_unet_dir).to(device)\n",
    "pipe.unet = merged_unet.to(device)\n",
    "pipe = pipe.to(device)\n",
    "\n",
    "prompt = \"A modern tech conference with people networking in a large exhibition hall, cinematic lighting\"\n",
    "generator = torch.Generator(device=device).manual_seed(42)\n",
    "out = pipe(prompt, num_inference_steps=25, guidance_scale=7.5, generator=generator, height=512, width=512)\n",
    "img = out.images[0]\n",
    "out_path = \"/content/lora_inference.png\"\n",
    "img.save(out_path)\n",
    "print(\"Saved inference image to:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2aa103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a187ae0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
